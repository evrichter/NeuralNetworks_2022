{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5E36qAPf-8f",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# NNIA Assignment 9\n",
        "\n",
        "**DEADLINE: 18. 01. 2023 08:00 CET**\n",
        "Submission more than 10 minutes past the deadline will **not** be graded!\n",
        "\n",
        "- **Name & ID 1**: Antonia WÃ¤chter, 7007542 (anwa00001)\n",
        "- **Name & ID 2**: Asmaa Ibrahim, 7039914 (asib00001)\n",
        "- **Name & ID 3**: Eva Richter, 2559531 (s8evrich)\n",
        "- Hours of work per person: 5\n",
        "\n",
        "# Submission Instructions\n",
        "\n",
        "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the assignment deadline. All course-related questions can be addressed on the course **[Piazza Platform](https://piazza.com/class/kvc3vzhsvh55rt)**.\n",
        "\n",
        "* Assignments are to be submitted in a **team of 2 or 3**.\n",
        "* Please include your **names**, **ID's**, **Teams usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
        "* Make sure you appropriately comment your code wherever required.\n",
        "* Your final submission should contain this completed Jupyter Notebook, including the bonus question (if you attempt it), and any necessary Python files.\n",
        "* Do **not** submit any **data or cache files** (e.g. `__pycache__`, the dataset PyTorch downloads, etc.). \n",
        "* Upload the **zipped** folder (*.zip* is the only accepted extension) in **Teams**.\n",
        "* Only **one member** of the group should make the submisssion.\n",
        "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2.zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization epeatedly students fail to do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv5YtUy8hrYv"
      },
      "source": [
        "## 1 Theory Review (3 pts)\n",
        "\n",
        "\n",
        "\n",
        "Review [chapter 9](https://www.deeplearningbook.org/contents/convnets.html) of the Deep Learning book and the lecture slides and answer the following questions: \n",
        "1. In your own words, how are the concepts of *sparse interactions*, *parameter sharing*, and *equivariant representations* applied in convolutional neural networks? *(1 pt)*\n",
        "1. What kinds of (zero-)padding schemes are there (as decribed in the book)? When should one scheme be chosen over another? (1 pt)\n",
        "1. How do CNNs handle inputs of varrying sizes? (0.5 pts)\n",
        "1. What are learned invariances and what effect do they have on the number of kernel parameters? (0.5 pts)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oQEOH4gckl2d"
      },
      "source": [
        "## <font color=\"green\">Done</font>\n",
        "\n",
        "\n",
        "### 1\n",
        "1.  Sparse interaction: In traditional neural networks, each output unit connects to every input unit. However, in convolutional networks, the interactions between neurons in different layers are typically sparse, achieved by using a smaller kernel than the input size.\n",
        "\n",
        "    Parameter sharing:\n",
        "    In convolutional neural networks, parameter sharing refers to the use of the same set of parameters, such as weights and biases, across different parts of the input. This means instead of learning separate parameters for every location, only one set is learned.\n",
        "\n",
        "    Equivariant representation:\n",
        "    With parameter sharing, convolutional layers have a property called equivariance to translation. This means that if the input changes, the output changes in the same way. Specifically, if the input is shifted by some function g, the convolution output will also shift in the same way as g. This ensures that the network's output will change in a predictable and consistent wa cwhen the input is transformed.\n",
        "    \n",
        "\n",
        "2. If no zero padding is used, we call it a valid convolution. If there is just enough zero padding added to keep the size of the output equal to the size of the input, we speak of same convolution. And the other extreme case, where enough zeros are added for every pixel to be visited k times in each direction is called full convolution. \n",
        "\n",
        "3. By using a process called pooling. A pooling function replaces the output of the net at a certain location with a statistic of the adjacent outputs (max pooling for example gives the maximum output within a rectangular neighborhood).\n",
        "\n",
        "4. Learned invariances refer to the ability of the network to learn to recognize patterns in the input data that are invariant to certain types of transformations or variations. The effect of learned invariances on the number of kernel parameters is that it reduces the number of parameters that are needed to be learned. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYPfZXJjDeKy"
      },
      "source": [
        "## 2 Output of a convolutional layer (3 pts)\n",
        "The following [blog post](https://www.baeldung.com/cs/convolutional-layer-size) may be helpful for this exercise.  \n",
        "Compute the output of a convonlutional layer given the kernel:  \n",
        "\\begin{array}{|l|l|}\n",
        "\\hline\n",
        "1 & 0 & 1 \\\\ \\hline\n",
        "0 & 1 & 0 \\\\ \\hline\n",
        "1 & 0 & 1 \\\\ \\hline\n",
        "\\end{array}  \n",
        "and the following input RGB image with three values in each cell - one for each channel:  \n",
        "  \n",
        "\\begin{array}{|c|c|c|c|}\n",
        "\\hline\n",
        "1, 2, 3 & 1, 1, 0 & 2, 0, 1 & 1, 1, 0 \\\\ \\hline\n",
        "0, 0, 0 & 1,3,1 & 0, 2, 1 & -1, -2, 0 \\\\ \\hline\n",
        "-1, 0, 0 & 1, 1, 2 & 0, 0, 0 & 1, 1, 0 \\\\ \\hline\n",
        "1, 1, 1 & 1, 0, 2 & 1, 3, -3 & 0, 0, 0 \\\\ \\hline\n",
        "\\end{array}  \n",
        "Use stride = 1 and padding = 0 and state the output dimensionality. What would the output dimensionality be if we used padding = 1?  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ljIs1noMkl2f"
      },
      "source": [
        "## <font color=\"green\">Done</font>\n",
        "\n",
        "\n",
        "### 2\n",
        "![ex2.jpeg](attachment:ex2.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RzDiyFDrF0J"
      },
      "source": [
        "## 3 Implementation (4 pts)\n",
        "\n",
        "### Prelude -- Using a validation dataset\n",
        "\n",
        "In this task we also officially introduce the use of a validation set in a homework assignment.\n",
        "\n",
        "A validation set allows you to due some further fine-tuning of your hyperparameters on a set of data that was not used for training. This has two purposes: 1) testing the generalization capabilities of your model and 2) verify that your model can handle unseen data as this may come in formats that you were not specifically expecting.\n",
        "\n",
        "Take a look at these articles to gain more insight into why we use validation sets in the world of Deep Learning:\n",
        "\n",
        "* [Why use both validation set and test set?](https://datascience.stackexchange.com/a/18346)\n",
        "* [Why Do We Need a Validation Set in Addition to Training and Test Sets?](https://towardsdatascience.com/why-do-we-need-a-validation-set-in-addition-to-training-and-test-sets-5cf4a65550e0)\n",
        "\n",
        "### Implementation task\n",
        "In this exercise, we will continue to work with [the PyTorch Datasets Class](https://pytorch.org/vision/stable/datasets.html) to obtain\n",
        "[the CIFAR100 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html). Instead of the simple neural network from the previous assignment, we are going to implement a convolution neural network (CNN) model to classify the images in this dataset into their proper classes.\n",
        "\n",
        "Your CNN model will have the following architecture:\n",
        "\n",
        "\n",
        "* It will have five convolution blocks. \n",
        "* Each block consists of the *convolution*, *max pooling* and *ReLU* operation in that order. \n",
        "* We will use $3\\times3$ kernels in all convolutional layers. Set the padding and stride of the convolutional layers so that they **maintain** the spatial dimensions. \n",
        "* Max pooling operations are done with $2\\times2$ kernels, with a stride of 2, thereby **halving** the spatial resolution each time. \n",
        "* Finally, five stacking these five blocks leads to a $512\\times1\\times1$ feature map. \n",
        "* Classification is achieved using a fully connected layer. \n",
        "\n",
        "Implement the class *ConvNet* to define the model described. The ConvNet model takes $32\\times32$ color images as inputs and has 5 hidden layers with 128, 512, 512, 512, 512 filters, and produces a 100-class classification. We will train the convolutional neural network on the CIFAR-100 dataset. Feel free to incorporate drop-put, batch normalization, and early stopping if desired. Evaluate your trained model on the test set and report your findings.\n",
        "\n",
        "For loss, you can use cross entropy loss and for optimization, you can use the Adam optimizer with the learning rate of `2e-3` and weight decay of $0.001$. \n",
        "       \n",
        "**Note**: To speed up trainining on the entire dataset, you may want an access to a GPU (CPU runtime > 10 hrs vs < 5 mins GPU). We recommend you make use of [Google Colab](https://colab.research.google.com/?utm_source=scs-index).  \n",
        "If you are having trouble loading the dataset [this post](https://stackoverflow.com/questions/71263622/sslcertverificationerror-when-downloading-pytorch-datasets-via-torchvision) on stackoverflow may be helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG5y_d8vrPeR"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKn5Jwr1Qr78"
      },
      "outputs": [],
      "source": [
        "# Import some libraries\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WilMrkWCUCdw"
      },
      "outputs": [],
      "source": [
        "# We recommend using a GPU for this task, if not available a CPU will be used\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DjoaSmQrQO7"
      },
      "outputs": [],
      "source": [
        "# Feel free to modify these variables how you deem fit,\n",
        "# as long as you are still following the above instructions\n",
        "# (e.g. you respect the proposed network architecture)\n",
        "LR = 2e-3\n",
        "REG = 0.001\n",
        "INPUT_SIZE = 3\n",
        "NUM_CLASSES = 100\n",
        "HIDDEN_SIZE = [128, 512, 512, 512, 512, 512]\n",
        "NUM_EPOCHS = 2\n",
        "BATCH_SIZE = 200\n",
        "LR_DECAY = 0.001\n",
        "TRAINING_SIZE = 0.8\n",
        "\n",
        "# Percentage of the training data to be used as validation data\n",
        "VAL_SIZE = 0.2\n",
        "DROP_OUT = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggC1IvHORjQW"
      },
      "outputs": [],
      "source": [
        "def get_cifar100_dataset(val_size=VAL_SIZE, batch_size=BATCH_SIZE):\n",
        "    \"\"\"\n",
        "    Load and transform the CIFAR100 dataset. Make Validation set. Create dataloaders for\n",
        "    train, test, validation sets.\n",
        "\n",
        "    NOTES:\n",
        "    1. DO NOT CHANGE THE CODE IN THIS FUNCTION. YOU MAY CHANGE THE BATCH_SIZE PARAM IF NEEDED.\n",
        "    2. If you get an error related `num_workers`, you may change that parameter to a different value.\n",
        "\n",
        "    :param val_size: size of the validation partition\n",
        "    :param batch_size: number of samples in a batch\n",
        "    :return: three data loaders and the set of possible classes\n",
        "    \"\"\"\n",
        "\n",
        "    # the datasets.CIFAR getitem actually returns img in PIL format\n",
        "    # no need to get to Tensor since we're working with our own model and not PyTorch\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.24703233, 0.24348505, 0.26158768))\n",
        "                                    ])\n",
        "\n",
        "    # Load the train_set and test_set from PyTorch, transform each sample to a flattened array\n",
        "    train_set = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                             download=True, transform=transform)\n",
        "    test_set = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                            download=True, transform=transform)\n",
        "    classes = train_set.classes\n",
        "\n",
        "    # Split data and define train_loader, test_loader, val_loader\n",
        "    val_size = int(len(train_set) * val_size)\n",
        "    train_size = len(train_set) - val_size\n",
        "    train_set, val_set = random_split(train_set, [train_size, val_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                               shuffle=True, num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
        "                                              shuffle=False, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, test_loader, val_loader, classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h5JRzcPSyW3",
        "outputId": "2564c455-5ba8-4883-834f-f86accad8a95",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "train_loader, test_loader, val_loader, classes = get_cifar100_dataset(val_size=VAL_SIZE, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0RR9PHXSrUM"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement the class `ConvNet`\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(INPUT_SIZE, HIDDEN_SIZE[0], 3, stride= 1, padding = 1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(HIDDEN_SIZE[0], HIDDEN_SIZE[1], 3, stride= 1, padding = 1)\n",
        "        self.conv3 = nn.Conv2d(HIDDEN_SIZE[1], HIDDEN_SIZE[2], 3, stride= 1, padding = 1)\n",
        "        self.conv4 = nn.Conv2d(HIDDEN_SIZE[2], HIDDEN_SIZE[3], 3, stride= 1, padding = 1)\n",
        "        self.conv5 = nn.Conv2d(HIDDEN_SIZE[3], HIDDEN_SIZE[4], 3, stride= 1, padding = 1)\n",
        "        self.fc1 = nn.Linear(512, 100)\n",
        "    # TODO: Implement the forward pass computations\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        #print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        #print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        #print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        #print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "        #print(x.shape)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = ConvNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdVC830eStN2",
        "outputId": "a75026fc-a5e1-4e4b-f818-d77d241a2ebf"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement the training function\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=LR , weight_decay=REG)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 50 == 49:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 50:.3f}')\n",
        "            running_loss = 0.0\n",
        "      \n",
        "    # Calculate validation loss at the end of each epoch\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for i, data in enumerate(val_loader):\n",
        "        #print(\"here\")\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "      print(f'After epoch {epoch + 1} validation loss: {val_loss / i:.3f}')\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc5MV9eMQ1d5"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement the evaluation function\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvGTnsyYGGru"
      },
      "source": [
        "## Bonus CNNs and NLP\n",
        "Research the web and discuss how CNNs can be applied to NLP tasks:   \n",
        "1. How is the input defined?\n",
        "2. What advantages do CNNs have over fully connected NNs?  \n",
        "3. For an RGB image input there are normally three channels - one for each color. What different channels can we consider given a language input?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw3cYxPQ_jFU"
      },
      "source": [
        "1. The input is a matrix where each row is a numerical representation of a token, which is usually a word but can also be a character.  \n",
        "2. The CNNs are allowed to extract features in the input independent of there position.  \n",
        "3. the channels can be different representations of features of the sentence. So one channel can be numerical representation of the words as discussed in 1, another channel could be used for part of speech, or probability of the word etc "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
