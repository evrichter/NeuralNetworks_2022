{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5E36qAPf-8f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NNIA Assignment 7\n",
    "\n",
    "**DEADLINE: 04. 01. 2022 08:00 CET**\n",
    "Submission more than 10 minutes past the deadline will **not** be graded!\n",
    "\n",
    "\n",
    "- **Name & ID 1**: Antonia WÃ¤chter, 7007542 (anwa00001)\n",
    "- **Name & ID 2**: Asmaa Ibrahim, 7039914 (asib00001)\n",
    "- **Name & ID 3**: Eva Richter, 2559531 (s8evrich)\n",
    "- **Hours of work per person: 10\n",
    "\n",
    "\n",
    "# Submission Instructions\n",
    "\n",
    "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the assignment deadline. All course-related questions can be addressed on the course **[Piazza Platform](https://piazza.com/class/kvc3vzhsvh55rt)**.\n",
    "\n",
    "* Assignments are to be submitted in a **team of 2 or 3**.\n",
    "* Please include your **names**, **ID's**, **Teams usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
    "* Make sure you appropriately comment your code wherever required.\n",
    "* Your final submission should contain this completed Jupyter Notebook, including the bonus question (if you attempt it), and any necessary Python files.\n",
    "* Do **not** submit any **data or cache files** (e.g. `__pycache__`, the dataset PyTorch downloads, etc.). \n",
    "* Upload the **zipped** folder (*.zip* is the only accepted extension) in **Teams**.\n",
    "* Only **one member** of the group should make the submisssion.\n",
    "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2.zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization epeatedly students fail to do this.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88JQRIxOr8ZW"
   },
   "source": [
    "## Notations (update: inner product at the last line)\n",
    "\n",
    "- ${A \\in \\mathbb{R}^{m \\times n}}$: capital letter for a matrix. ${m}$ refers to the number of rows and ${n}$ refers to the number of columns.\n",
    "- ${\\textbf{x} \\in \\mathbb{R}^{n \\times 1}}$: bold lowercase for a vector\n",
    "- ${a \\in \\mathbb{R}}$: lowercase for a scalar\n",
    "- ${\\textbf{1}}$: bold one denotes a vector of ones, e.g., ${a \\cdot \\textbf{1} = a \\textbf{1} = [a \\cdot 1,~a \\cdot 1]^{T} = \\begin{bmatrix}a\\\\a\\end{bmatrix} }$.\\\n",
    "Superscipt ${T}$ means \"transpose\": swap rows and columns.\n",
    "- Symbol ${:=}$ means, \"(left-hand side) is defined as (right-hand side)\".\n",
    "- ${f(w;x, y)}$ means that we see it as a function of ${w}$, while ${x}$ and ${y}$ are given (and fixed). Variable ${w}$ can be a trainable parameter in a neural network model, while ${x}$ is an input data and ${y}$ is the ground truth solution for the data point ${x}$. It means that we are going to update ${w}$ during back-propagation, while both ${x}$ and ${y}$ are given (and fixed) data.\n",
    "- Symbol ${\\rightarrow}$ means that the right-hand side of ${\\rightarrow}$ is computed by some operations given the left-hand side.\n",
    "- Symbol ${\\leftrightarrow}$ means an expression on the right-hand side of ${\\leftrightarrow}$ is equivalent to the left-hand side (see below).\n",
    "- Symbol ${ \\circ }$ means a composition of functions: A function on the right-hand side of ${\\circ}$ serves as an input argument to a function on the left-hand side. So, it holds that \\begin{align*} g \\circ f(x) ~\\leftrightarrow~ g(f(x)). \\end{align*}\n",
    "- [Gradient](https://en.wikipedia.org/wiki/Gradient) is generalisation of derivative for a function in higher dimension (many parameters). For example, a function \\begin{align*} f(x_{1}, x_{2}, ..., x_{n}) \\end{align*} may have its gradient \\begin{align*} \\nabla f(x_{1}, x_{2}, ..., x_{n}) = \\left[~\\frac{\\partial}{\\partial x_{1}} f, \\frac{\\partial}{\\partial x_{2}} f, ..., \\frac{\\partial}{\\partial x_{n}} f \\right]^{T}. \\end{align*} Each element in a gradient vector is a [partial derivative](https://en.wikipedia.org/wiki/Partial_derivative).\n",
    "\n",
    "## Definitions and Properties\n",
    "- ${\\ell_{2}}$-norm: ${ {\\lVert{\\textbf{x}}\\rVert}_{2} := \\sqrt{  \\sum_{i = 1}^{n} \\lvert{ x_{i} }^{2} }\\rvert = \\sqrt{ x_{1}^{2} + ..... + x_{n}^{2} } }$\n",
    "- squared ${\\ell_{2}}$-norm: ${ \\lVert{\\textbf{x}}\\rVert_{2}^{2} = x_{1}^{2} + ... + x_{n}^{2} }$\n",
    "- inner product: ${\\textbf{x}^{T} \\textbf{x} = \\langle\\textbf{x},\\textbf{x}\\rangle} = x_{1}^{2} + x_{2}^{2} + ... + x_{n-1}^{2} + x_{n}^{2} = \\lVert{\\textbf{x}}\\rVert_{2}^{2}$\\\n",
    "The above is for a vector ${\\textbf{x}}$.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQTA6qj7pqRw"
   },
   "source": [
    "## 1. Getting Started with PyTorch in Google Collab (with Free GPU)\n",
    "\n",
    "PyTorch: A Deep Learning Framework\n",
    "- It shares many commands with Numpy.\n",
    "- It supports arbitrary-dimensional tensor in GPU.\n",
    "- It supports automatic differentiation for common neural-network functions.\n",
    "- PyTorch was initially developed by Facebook (now Meta) and Uber.\n",
    "\n",
    "Google Colab: A Research / Education tool for Machine Learning\n",
    "- It is an online Jupyter notebook with free GPU support (for a limited time <= 12 hours consecutively).\n",
    "\n",
    "In Google Colab, PyTorch is already installed, along side with other many commonly used Python packages for machine learning.\n",
    "So, you don't need to set up your neural-network and machine learning environment at your personal computer for educational purposes: Just use Google Colab via your browser.\n",
    "\n",
    "References\n",
    "\n",
    "https://www.marktechpost.com/2021/01/09/getting-started-with-pytorch-in-google-collab-with-free-gpu/\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mo0JFjTIqi_c"
   },
   "source": [
    "### 1.1 Let's run this Jupyter notebook (.ipynb) in Google Colab\n",
    "- Upload this Jupyter notebook file in your private Google Drive.\n",
    "- Double-click to run this Jupyter notebook in Google Drive.\n",
    "- Once opened, you will see *connect* at the top-right.\n",
    "  Click *connect to hosted runtime*; this will launch your Jupyter notebook file at a Google server.\n",
    "- Browse the tab: Runtime >> Change runtime type (click it). Choose GPU as *hardware accellerator*.\n",
    "- Run the code snippet below to ensure that you have access to a GPU at Google server.\n",
    "- Carefully read the example code below to check how you can access matrix elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Let's check the connected GPU.\n",
    "\n",
    "print('%s #-of devices: %d, CUDA available: %s' \n",
    "      %(torch.cuda.get_device_name(0), \n",
    "        torch.cuda.device_count(),\n",
    "        torch.cuda.is_available()))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\", end = '\\n\\n')\n",
    "\n",
    "# How do we assign matrices and vectors (tensors) in GPU or in CPU memory?\n",
    "\n",
    "dim1 = 3; dim2 = 2\n",
    "\n",
    "CUDA0 = torch.device('cuda:0')\n",
    "A = torch.rand(dim1, dim2, device = CUDA0)  # a tensor at GPU\n",
    "B = torch.rand(dim1, dim2, device = CUDA0)\n",
    "C = A + B                                   # The operation is done at GPU.\n",
    "D = C.cpu()                                 # Bring the result C to CPU memory.\n",
    "print(C - D.to(CUDA0), end = '\\n\\n')        # PyTorch.Tensor.to(GPU)\n",
    "\n",
    "I4 = torch.eye(3) * 4                       # 4 I: You will need this later.\n",
    "print(I4)\n",
    "\n",
    "I4[0, 1] = 2                                # How to access elements\n",
    "I4[:, 2] = 1                                # Slicing\n",
    "print('\\n', I4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PKT1Uc0tb7u"
   },
   "source": [
    "Now you are prepared to start with the remaining exercises.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8CcQv1vjUFp"
   },
   "source": [
    "## 2. ${L_{2}}$ Regularisation\n",
    "\n",
    "In this exercise, we will be learning the effect of \n",
    "${L_{2}}$ regularisation with the simplest possible\n",
    "neural network model; namely, one hidden layer even\n",
    "without a nonlinear activation function, minimising\n",
    "mean squared error as loss function.\n",
    "<br>\n",
    "\n",
    "See differences between (1) and (2).\n",
    "They only differ by an ${L_{2}}$-regularisation\n",
    "term: ${{\\lVert \\textbf{w} \\rVert}_{2}^{2}}$.\n",
    "<br><br>\n",
    "\n",
    "\\begin{align}\n",
    "\\ell \\left( \\textbf{w}; X, \\textbf{y} \\right) &:= \\frac{1}{n} {{\\lVert \\hat{\\textbf{y}} - \\textbf{y} \\rVert}_{2}^{2},} && (1)\\\\\n",
    "L \\left( \\textbf{w}; X, \\textbf{y} \\right) &:= \\frac{1}{n} {{\\lVert \\hat{\\textbf{y}} - \\textbf{y} \\rVert}_{2}^{2} \n",
    "+ \\lambda {\\lVert \\textbf{w} \\rVert}_{2}^{2}, } && (2)\n",
    "\\end{align}\n",
    "with\n",
    "\\begin{align}\n",
    "\\hat{\\textbf{y}} = X \\textbf{w}. &&(3)\n",
    "\\end{align}\n",
    "<br>\n",
    "\n",
    "Given an input feature matrix ${X \\in \\mathbb{R}^{n \\times d}}$ and its ground truth solution ${\\textbf{y} \\in \\mathbb{R}^{n \\times 1}}$, our goal is to to find the best parameters ${\\textbf{w} \\in \\mathbb{R}^{d \\times 1}}$. Here, ${n}$ refers to the number of training examples, and ${d}$ refers to the dimension of the input features. So the matrix-vector multiplication results in\n",
    "\n",
    "\\begin{align}\n",
    "\\textbf{y}, \\hat{\\textbf{y}} \\in \\mathbb{R}^{n \\times 1}. && \\left( \\mathbb{R}^{n \\times d} \\times \\mathbb{R}^{d \\times 1} \\right)\n",
    "\\end{align}\n",
    "<br>\n",
    "\n",
    "In machine learning, (1) is called *linear regression* due to the fact that the prediction relies only on a linear operation (3). Adding ${L_{2}}$-regularisation term as in (2) on top of this is called *ridge regression*. Such ${L_{2}}$ regularisation is not limited to linear regression model; so we will learn what they do with this simple predictor ${\\hat{\\textbf{y}} = X \\textbf{w}}$, such that you will be able to confidently use this technique also in neural networks in future.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGXXAeWiuoex"
   },
   "source": [
    "### 2.1 How does the linear operation look like? (+0.5 pts)\n",
    "\n",
    "Consider the case where ${X \\in \\mathbb{R}^{n \\times 1}}$ (remind that vector is a special case of matrix, so please don't get alarmed by capital ${X}$). It means that we have ${n}$ number of data points with only one feature dimension (d = 1). For example, your input data ${\\textbf{x} \\in \\mathbb{R}^{n \\times 1}}$ can be sampling time (sec) of an acoustic signal ${\\textbf{y} \\in \\mathbb{R}^{n \\times 1}}$.\n",
    "<br>\n",
    "\n",
    "- Explicitly write down elements in vector\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\textbf{y}} = X \\textbf{w}\n",
    "\\end{align}\n",
    "\n",
    "for this particular case: ${d = 1}$.\n",
    "You can use ${...}$ to simplify elements in the middle.\n",
    "<br>\n",
    "\n",
    "Your input data looks like\n",
    "\\begin{align}\n",
    "\\textbf{X} = \\begin{bmatrix}\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "...\\\\\n",
    "x_{n-1}\\\\\n",
    "x_{n}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "<br>\n",
    "\n",
    "Hint. Your weight (trainable parameter) vector reduces to a scalar \n",
    "${ \\textbf{w} = [w] \\in \\mathbb{R} }$ because ${d = 1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2GaX5BPv2TV"
   },
   "source": [
    "___\n",
    "### 2.1 <font color=\"green\">Done</font>\n",
    "\n",
    "$\\bf{\\hat{y}} = \\begin{bmatrix}x_{1}w\\\\x_{2}w\\\\...\\\\x_{n-1}w\\\\x_{n}w\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAERZd5-wEi2"
   },
   "source": [
    "### 2.2 Let us improve the model capacity. (+0.5 pts)\n",
    "\n",
    "If we consider\n",
    "\\begin{align}\n",
    "f(x) := w_{0} \\cdot x + 1 \\cdot w_{1},\n",
    "\\end{align}\n",
    "\n",
    "one calls ${w_{0}}$ slope and ${w_{1}}$ bias.\n",
    "\n",
    "For ${d = 1}$ case in 2.1, we do have the slope (for our input data ${\\textbf{x}}$) but we are missing the bias. \n",
    "<br>\n",
    "\n",
    "- Explicitly write down elements in matrix ${X \\in \\mathbb{R}^{n \\times 2}}$ where we want to introduce such a missing bias term in our model ${(d = 2)}$. You can use ${...}$ to simplify elements in the middle from 2.1 to 2.4. \n",
    "\n",
    "Hint. Think about how ${\\textbf{w} \\in \\mathbb{R}^{2 \\times 1}}$ looks like and what should happen with the linear operation ${X \\textbf{w}}$. You also can go back and consult section Notations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRZRrsXXx_1S"
   },
   "source": [
    "___\n",
    "### 2.2 <font color=\"green\">Done</font>\n",
    "\n",
    "$\\bf{X} = \\begin{bmatrix}x_{1} & 1\\\\x_{2} & 1\\\\... & ...\\\\x_{n-1} & 1\\\\x_{n} & 1\\end{bmatrix}$, \n",
    "$\\bf{w} = \\begin{bmatrix}w_{0}\\\\w_{1}\\end{bmatrix}$, $\\bf{Xw} = \\begin{bmatrix}x_{1}w_{0} + 1w_{1}\\\\x_{2}w_0 + 1w_{1}\\\\... + ...\\\\x_{n-1}w_{0} + 1w_{1}\\\\x_{n}w_{0} + 1w_{1}\\end{bmatrix}$\n",
    "\n",
    "$\\textbf{X} = nx2, \\textbf{w} = 2x1, \\textbf{Xw} = nx1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf9RD-f2yFRW"
   },
   "source": [
    "### 2.3 Improving the model capacity further. (+0.5 pts)\n",
    "Assume that you got a hunch that your ground truth data ${\\textbf{y} \\in \\mathbb{R}^{n}}$ is going to be well-described by a quadratic function:\n",
    "<br>\n",
    "\n",
    "\\begin{align}\n",
    "g(x) := w_{0} \\cdot x^{2} + w_{1} \\cdot x + w_{0}.\n",
    "\\end{align}\n",
    "\n",
    "Explicitly write down elements in matrix ${X \\in \\mathbb{R}^{n \\times 3}}$ to model your problem as in ${g(x)}$.\n",
    "\n",
    "Hint. \"The amplitude g(x) at time x is described as...\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq0pNaOnzSfh"
   },
   "source": [
    "___\n",
    "### 2.3 <font color=\"green\">Done</font>\n",
    "\n",
    "$\\textbf{X} = \\begin{bmatrix}x_{1}^{2} & x_{1} & 1\\\\x_{2}^{2} & x_{2} & 1\\\\... & ... & ...\\\\x_{n-1}^{2} & x_{n-1} & 1\\\\x_{n}^{2} & x_{n} & 1\\end{bmatrix}$, \n",
    "$\\bf{w} = \\begin{bmatrix}w_{0}\\\\w_{1}\\\\w_{2}\\end{bmatrix}$, $\\textbf{Xw} = \\begin{bmatrix}x_{1}^{2}w_0 + x_{1}w_{1} + 1w_{2}\\\\x_{2}^{2}w_{0} + x_{2}w_{1} + 1w_{2}\\\\... + ... + ...\\\\x_{n-1}^{2}w_{0} + x_{n-1}w_{1} + 1w_{2}\\\\x_{n}^{2}w_{0} + x_{n}w_{1} + 1w_{2}\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9rc_tOazUGa"
   },
   "source": [
    "### 2.4 Generalisation to an arbitrary polynomial (+0.5 pts)\n",
    "\n",
    "In 2.3, our model is a quadratic function (as a predictor). Let us denote the order of this polynomial function with ${p = 2}$ (in 2.3).\n",
    "\n",
    "Explicitly write down the elements in the feature matrix ${X \\in \\mathbb{R}^{n \\times (p + 1)}}$ for a ${p}$-th order polynomial model (including the bias term). You will need this to commence with the implementation below.\n",
    "\n",
    "Hint. ${h(x) = w_{0} \\cdot x^{p} + w_{1} \\cdot x^{p-1} + ... + w_{p} \\cdot x^{0}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CVObdLz0EA6"
   },
   "source": [
    "___\n",
    "### 2.4 <font color=\"green\">Done</font>\n",
    "\n",
    "$\\textbf{X} = \\begin{bmatrix}x_{1}^{p} & x_{1}^{p-1} & ... & x_{1} & 1\\\\x_{2}^{p} & x_{2}^{p-1} & ... & x_{2} & 1\\\\... & ... & ... & ...& ...\\\\x_{n-1}^{p} & x_{n-1}^{p-1} & ... & x_{n-1}&1\\\\x_{n}^{p} & x_{n}^{p-1} & ... & x_{n} &1\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYy88Ig33gkI"
   },
   "source": [
    "### 2.5 Implementing Ridge Regression with PyTorch (+2 pts)\n",
    "\n",
    "Ridge regression minimises the following loss function:\n",
    "\n",
    "\\begin{align}\n",
    "L \\left( \\textbf{w}; X, \\textbf{y} \\right) := \\frac{1}{n} {{\\lVert X \\textbf{w} - \\textbf{y} \\rVert}_{2}^{2} + \\lambda {\\lVert \\textbf{w} \\rVert}_{2}^{2}. } && (2)\n",
    "\\end{align}\n",
    "<br>\n",
    "\n",
    "The globally optimal solution for your training parameter ${\\textbf{w}}$ is given as\n",
    "\n",
    "\\begin{align}\n",
    "\\textbf{w} = \\left( X^{T} X + \\lambda I \\right)^{-1} \\left(X^{T}\\textbf{y} \\right). && (4)\n",
    "\\end{align}\n",
    "<br>\n",
    "\n",
    "where ${X^{T}}$ is the [transpose](https://en.wikipedia.org/wiki/Transpose) of your feature matrix ${X}$, and ${X^{T} X} \\in \\mathbb{R}^{d \\times d}$ is a matrix-matrix multiplication. The regularisation parameter ${\\lambda \\geq 0}$ is a scalar that you choose (hyperparameter). Finally, ${I}$ denotes an [identity matrix](https://en.wikipedia.org/wiki/Identity_matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IE0VDyxOluQy"
   },
   "source": [
    "Let us implement ridge regression and study impact of choosing different ${L_{2}}$-regularisation parameter ${\\lambda}$. Don't forget that lambda is a non-negative value. We are going to minimise the loss function (2) w.r.t. the trainable parameter ${\\textbf{w}}$ by using the analytic solution (4); we do not need gradient iterations in this case because we know the global optimal solution's analytic expression (4) and the problem size is small. \n",
    "\n",
    "<br>\n",
    "Hint: By analytic, we mean that we know the form of the solution but it still requires to solve a system of equations.\n",
    "<br>\n",
    "\n",
    "In the following, input vector ${\\textbf{y}}$ is an acoustic signal (1D) over time ${\\textbf{x}}$.\n",
    "It looks very noisy. We want to find a good ${\\hat{\\textbf{y}} = X \\textbf{w}}$\n",
    "with ridge regression in this exercise.\n",
    "<br>\n",
    "\n",
    "### Use [torch.transpose()](https://pytorch.org/docs/stable/generated/torch.transpose.html) [torch.matmul()](https://pytorch.org/docs/stable/generated/torch.matmul.html) [torch.linalg.solve()](https://pytorch.org/docs/stable/generated/torch.linalg.solve.html) to compute (4).\n",
    "\n",
    "- w = torch.linalg.solve(X, y) ${\\leftrightarrow}$\n",
    "  ${\\textbf{w} = X^{-1} \\textbf{y}}$.\n",
    "- Implement ridge regression for the data below. We ask you to use the highest polynomial order ${p = 19}$ based on your solution in 2.4, and there must be a bias term; this means that the dimension of your feature matrix is going to be ${X \\in \\mathbb{R}^{256 \\times 20}}$. \n",
    "- It is not a mandatory, but try to load matrix and vectors (tensors) at GPU so that linear-algebraic operations also happen at GPU (PyTorch.Tensor); so that you can freely use GPU capability later as you wish with PyTorch.\n",
    "- Try three different regularisation parameters ${\\lambda = \\{ 0, 1e^{-3}, 1 \\}}$\n",
    "and store their predictions ${\\hat{\\textbf{y}} = X \\textbf{w}}$ separately for each lambda.\n",
    "- Plot ${\\hat{\\textbf{y}} = X \\textbf{w}}$ for different lambda in the same figure and compare to the given acoustic signal ${\\textbf{y}}$. So, there must be four graphs in the same figure.\n",
    "What is your observation? Discuss your ideas in detail. Can you relate your observation to overfitting and underfitting?\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIVEN DATA FOR STUDENTS (just run this section)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = np.array([1.32770000000000, 1.33732000000000, 1.33732000000000, 1.30846000000000, 1.33090000000000, 1.34694000000000, 1.33412000000000, 1.33091000000000, 1.34053000000000, 1.31167000000000, 1.32449000000000, 1.29563000000000, 1.34053000000000, 1.30846000000000, 1.32129000000000, 1.30525000000000, 1.30846000000000, 1.31487000000000, 1.32449000000000, 1.28601000000000, 1.27639000000000, 1.30846000000000, 1.29563000000000, 1.27639000000000, 1.28280000000000, 1.27639000000000, 1.26677000000000, 1.28280000000000, 1.26035000000000, 1.30205000000000, 1.26677000000000, 1.28280000000000, 1.29563000000000, 1.25715000000000, 1.27639000000000, 1.29884000000000, 1.27639000000000, 1.28280000000000, 1.29884000000000, 1.26998000000000, 1.28922000000000, 1.26356000000000, 1.28922000000000, 1.29884000000000, 1.29564000000000, 1.28922000000000, 1.26677000000000, 1.27318000000000, 1.27960000000000, 1.26356000000000, 1.26677000000000, 1.32129000000000, 1.29564000000000, 1.27639000000000, 1.26037000000000, 1.31808000000000, 1.26679000000000, 1.30526000000000, 1.31809000000000, 1.28922000000000, 1.27960000000000, 1.32130000000000, 1.32451000000000, 1.28602000000000, 1.31488000000000, 1.30205000000000, 1.31810000000000, 1.31168000000000, 1.31489000000000, 1.33092000000000, 1.27961000000000, 1.29886000000000, 1.31489000000000, 1.29886000000000, 1.30847000000000, 1.30527000000000, 1.32131000000000, 1.30206000000000, 1.30527000000000, 1.30527000000000, 1.28924000000000, 1.29565000000000, 1.31489000000000, 1.31168000000000, 1.27961000000000, 1.31489000000000, 1.32131000000000, 1.28603000000000, 1.31168000000000, 1.32772000000000, 1.31489000000000, 1.30848000000000, 1.31489000000000, 1.29565000000000, 1.29886000000000, 1.28603000000000, 1.34376000000000, 1.30206000000000, 1.31489000000000, 1.33093000000000, 1.29565000000000, 1.30848000000000, 1.35658000000000, 1.32131000000000, 1.33413000000000, 1.31810000000000, 1.44000000000000, 1.45000000000000, 1.46000000000000, 1.45000000000000, 1.44000000000000, 1.29565000000000, 1.34055000000000, 1.33734000000000, 1.33093000000000, 1.30848000000000, 1.32131000000000, 1.32451000000000, 1.30527000000000, 1.26679000000000, 1.30528000000000, 1.31169000000000, 1.28283000000000, 1.29244000000000, 1.31168000000000, 1.29245000000000, 1.31169000000000, 1.29244000000000, 1.29565000000000, 1.28603000000000, 1.28282000000000, 1.25396000000000, 1.31489000000000, 1.26358000000000, 1.29566000000000, 1.26679000000000, 1.29887000000000, 1.27641000000000, 1.27641000000000, 1.27642000000000, 1.28925000000000, 1.27642000000000, 1.28603000000000, 1.29887000000000, 1.28284000000000, 1.27641000000000, 1.28925000000000, 1.29887000000000, 1.28282000000000, 1.29246000000000, 1.31170000000000, 1.28604000000000, 1.26680000000000, 1.29246000000000, 1.30850000000000, 1.29246000000000, 1.29246000000000, 1.28925000000000, 1.30850000000000, 1.26360000000000, 1.31170000000000, 1.30849000000000, 1.30208000000000, 1.29246000000000, 1.32453000000000, 1.32774000000000, 1.26360000000000, 1.32453000000000, 1.33415000000000, 1.26680000000000, 1.30850000000000, 1.33736000000000, 1.31170000000000, 1.31170000000000, 1.30208000000000, 1.32453000000000, 1.29887000000000, 1.26680000000000, 1.31812000000000, 1.31170000000000, 1.30850000000000, 1.30529000000000, 1.32774000000000, 1.31491000000000, 1.28605000000000, 1.29887000000000, 1.33736000000000, 1.29887000000000, 1.31170000000000, 1.30529000000000, 1.30208000000000, 1.30208000000000, 1.29887000000000, 1.32453000000000, 1.29567000000000, 1.29567000000000, 1.30529000000000, 1.30850000000000, 1.30208000000000, 1.30209000000000, 1.28284000000000, 1.32774000000000, 1.28925000000000, 1.31171000000000, 1.31491000000000, 1.34057000000000, 1.33738000000000, 1.31812000000000, 1.34378000000000, 1.31492000000000, 1.32454000000000, 1.33415000000000, 1.35020000000000, 1.30851000000000, 1.32775000000000, 1.31813000000000, 1.31492000000000, 1.31813000000000, 1.34058000000000, 1.31813000000000, 1.30850000000000, 1.29888000000000, 1.34698000000000, 1.31172000000000, 1.27323000000000, 1.33095000000000, 1.29888000000000, 1.29568000000000, 1.26682000000000, 1.32776000000000, 1.30210000000000, 1.28286000000000, 1.27644000000000, 1.30851000000000, 1.28286000000000, 1.23475000000000, 1.25720000000000, 1.29568000000000, 1.27003000000000, 1.26361000000000, 1.29248000000000, 1.28606000000000, 1.29248000000000, 1.26682000000000, 1.29889000000000, 1.30210000000000, 1.28927000000000, 1.29248000000000, 1.28927000000000, 1.29568000000000, 1.27323000000000, 1.28286000000000, 1.27644000000000, 1.29248000000000, 1.26361000000000, 1.30210000000000])\n",
    "y = torch.from_numpy(y).type(torch.float).to(CUDA0)\n",
    "n = y.size()[0]\n",
    "x = torch.linspace(-1, +1, n)\n",
    "\n",
    "print('\\nThere are %d number of data sampling points in vector y.\\n' %(len(x)))\n",
    "\n",
    "plt.figure(figsize=(11, 6), dpi=80)\n",
    "plt.plot(x.cpu().numpy(), y.cpu().numpy(),    color = 'grey')\n",
    "plt.scatter(x.cpu().numpy(), y.cpu().numpy(), color = 'purple', s = 10)\n",
    "plt.title('Acoustic Signal')\n",
    "plt.xlabel('Time: x')\n",
    "plt.ylabel('Amplitude: y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeCFz-rrTARO"
   },
   "source": [
    "### 2.5 <font color=\"green\">Done</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def ridge_regression(lambda_param):\n",
    "\n",
    "  trans = PolynomialFeatures(degree=19)  #add polynomials (of order p = 19) to the regression \n",
    "  data_reshaped = x.reshape(-1, 1) \n",
    "  data_x = trans.fit_transform(data_reshaped)\n",
    "\n",
    "  X = torch.tensor(data_x).to(CUDA0) #send to GPU\n",
    "\n",
    "  I = (torch.eye(20) * 1).to(CUDA0)  #create identity matrix with a one on the diagonal \n",
    "\n",
    "  w = torch.inverse((torch.transpose(X,0,1) @ X) + lambda_param * I) @ (torch.transpose(X, 0, 1) @ y.double().to(CUDA0)) #compute weights\n",
    "  \n",
    "  y_hat = X@w #compute predictions\n",
    "\n",
    "  return y_hat \n",
    "\n",
    "\n",
    "#Try different values for lambda\n",
    "\n",
    "y_hat1 = ridge_regression(0)\n",
    "y_hat2 = ridge_regression(0.001)\n",
    "y_hat3 = ridge_regression(1)\n",
    "\n",
    "x = x.cpu()  #send back to CPU\n",
    "y = y.cpu()  #send back to CPU\n",
    "\n",
    "#Plot predictions for all 3 lambdas\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize = (16, 8))\n",
    "\n",
    "axs[0, 0].plot(x, y, 'tab:blue')\n",
    "axs[0, 0].set_title('Target (y)')\n",
    "\n",
    "axs[0, 1].plot(x, y_hat1.cpu(), 'tab:orange')\n",
    "axs[0, 1].set_title('lambda = 0'\n",
    "\n",
    "axs[1, 0].plot(x, y_hat2.cpu(), 'tab:red')\n",
    "axs[1, 0].set_title('lambda = 0.001')\n",
    "\n",
    "axs[1, 1].plot(x, y_hat3.cpu(), 'tab:purple')\n",
    "axs[1, 1].set_title('lambda = 1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that when lambda = 0, $\\hat{y}$ is too close to y, indicating overfitting, which usually leads to undesirable generalisation difficulties on unseen data. To prevent this, the so-called L2/Ridge regression method introduces a \"bias\", which is our lambda multiplied by the squared slope. Accordingly, a big penalty (bias) is introduced when the squared slope is multiplied by a larger lambda value. If lambda is 0, as in the second plot, it means that the penalty is 0, i.e. only the sum of squared errors is minimised. However, if lambda is increased, $\\hat{y}$ shows less sensitivity towards the input x. On the other hand, if the bias is too large, this can lead to the other extreme, which is shown in the last plot for lambda = 1. Since in this case lambda is too large, the function does not capture the relationship between input and output variable well enough. To prevent this, lambda should be reduced, i.e. the bias should be reduced or more variance should be introduced. In this case, our lambda value of 0.001 seems to be a good try, as the function is neither overfitting nor underfitting and thus maintains a good balance between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFQrxYuJn3WS"
   },
   "source": [
    "___\n",
    "- Bonus 1: Draw contour lines of ${L_{2}}$-norm and ${L_{1}}$-norm in 2D space: ${\\textbf{x} \\in \\mathbb{R}^{2}}$. One can use [matplotlib.pyplot.contour()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contour.html).\n",
    "- Bonus 2: Have a look into the differences between ${L_{2}}$-norm and ${L_{1}}$-norm's shapes in [2D space](https://miro.medium.com/max/1100/1*_e8BLNA749W_7yxi7hz-DA.gif); contour lines denote the same function value. \n",
    "What is the major use case of ${L_{1}}$ regularisation? How can they be useful?\n",
    "- Bonus 3: In our ridge regression model, we used a polynomial expressions to come up with a feature matrix (or design matrix) could one use some other functions than polynomials? What could be the analogy of such polynomials in the case of neural network models? Can one use cosine or sine functions instead? Justify your answer.\n",
    "\n",
    "*[reference](https://satishkumarmoparthi.medium.com/why-l1-norm-creates-sparsity-compared-with-l2-norm-3c6fa9c607f4#:~:text=The%20reason%20for%20using%20the,and%20thus%20a%20sparse%20solution.)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TAXUlcY2z6v"
   },
   "source": [
    "# 3 Dropout\n",
    "##3.1 Introduction to Dropout (1 pt)\n",
    "\n",
    "Read sections 1, 2 and 4 from [an introductory paper to dropout](https://dl.acm.org/doi/pdf/10.5555/2627435.2670313). Describe in detail what dropout is:\n",
    "- What is the statistical rationale behind it, why we need it (0.25)\n",
    "- How it is implemented (0.25)\n",
    "- Importantly what do we have to take care of during inference? (0.5)\n",
    "\n",
    "PS Did you know that dropout is technically [patented by Google](https://patents.google.com/patent/US9406017B2/en)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropout aims to avoid both the problem of overfitting (due to lack of data) and the problem of computational expensive computations. Lack of data is one of the main problems in machine learning that can lead, for example, to overfitting, i.e. the model adapts too much to the training data set so that generalisation to unseen test data is not possible. This could be prevented by regularising the models by averaging the predictions of all possible parameter settings and weighting each setting by the posterior probability implied in the training data. However, this requires an unmanageable amount of computing power. The dropout technique avoids both the problem of overfitting and computationally expensive calculations by randomly dropping out units in a neural network.\n",
    "\n",
    "- The dropout technique temporarily drops or removes certain neurons in the network during training, so that only a fraction of the weights is learned instead of all weights. Since after each iteration different neurons are kept and dropped out, this prevents that only few neurons influence the training process and therefore helps to reduce overfitting. For training with dropout a term $\\textit{r}$ is introduced which keeps the neuron turned on or turns it off by assigning 1 (turned on) or 0 (turned off) to it. Whether units are kept or dropped out depends on a probability that can be chosen using a validation set or just set to a probability value, usually 0.5.  The optimal probability for input units to be retained should, however, be closer to 1 than to 0. After applying dropout we end up with a âthinnedâ network, which consists of all the units that were kept after the dropout. For a network with $\\textit{n}$ units, there are in total $2^{n}$ possible thinned neural networks. Since for each training case a new thinned neural network is sampled and trained, dropout can be thought of as a collection of these $2^{n}$ thinned networks with shared weights, each of which gets trained rarely or never.\n",
    "\n",
    "- During inference, dropout should not be used because it is not feasible to average the predictions of many thinned models for test data. Instead, an approximate averaging method is often used, where a single neural network without dropout is used for testing. In this case, the weights of the units held at a certain probability $\\textit{p}$ during training are multiplied by this probability $\\textit{p}$ during testing to ensure equality of the expected output at the hidden units during training and the actual output during testing. In this way, we can combine $2^{n}$  nets with common weights into a single net used in testing. Using this method can in many cases help neural networks perform better on the test set due to a lower generalisation error compared to other regularisation methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLc3HRXw2yQG"
   },
   "source": [
    "## 3.2 Dropout in Pytorch (4 pts)\n",
    "Now that we understand how dropout works, let's implement a network in pytorch that utilizes [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html). We will be working with the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset.\n",
    "Implement the following network:\n",
    "<br><br>\n",
    "\n",
    "$L_{64 \\rightarrow 10} \\circ \\text{ReLU} \\circ L_{128 \\rightarrow 64} \\circ \\text{ReLU} \\circ L_{28\\cdot 28\\rightarrow 128}$\n",
    "<br><br>\n",
    "\n",
    "and apply dropout at rate of $0.5$ to the first two linear layers: ${L_{28\\cdot 28\\rightarrow 128}}$ and ${L_{128 \\rightarrow 64}}$. \n",
    "<br><br>\n",
    "\n",
    "As an optimizer, use Adam with a learning rate of $1e^{-4}$ and use [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) as the loss criterion. \n",
    "<br><br>\n",
    "\n",
    "Train your network on the MNIST dataset for 5 epochs with a batch size of 200 and print the accuracy of your model on the test set after each epoch. We will use the default train test split (see the pytorch [MNIST documentation](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html) for more information).\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "T5XC1EKb5x0q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Do not change\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# load data\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=200, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=200, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "acx5-rdD6Cg0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Training Loss after epoch     1: 1.830\n",
      "Test Accuracy after epoch     1: 0.626\n",
      "Starting epoch 2\n",
      "Training Loss after epoch     2: 1.008\n",
      "Test Accuracy after epoch     2: 0.742\n",
      "Starting epoch 3\n",
      "Training Loss after epoch     3: 0.760\n",
      "Test Accuracy after epoch     3: 0.801\n",
      "Starting epoch 4\n",
      "Training Loss after epoch     4: 0.645\n",
      "Test Accuracy after epoch     4: 0.824\n",
      "Starting epoch 5\n",
      "Training Loss after epoch     5: 0.576\n",
      "Test Accuracy after epoch     5: 0.838\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "#SOLUTION ASMA\n",
    "# your code here\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "  '''\n",
    "    The network\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(28 * 28 * 1, 128),      \n",
    "      nn.Dropout(p=0.5),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 64),\n",
    "      nn.Dropout(p=0.5),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 10)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "  \n",
    "  \n",
    "  \n",
    "# Initialize the network\n",
    "network = Network()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "\n",
    "# Run the training loop for 5 epochs\n",
    "for epoch in range(0, 5): \n",
    "  \n",
    "  print(f'Starting epoch {epoch+1}')\n",
    "  \n",
    "  # Set current loss value\n",
    "  current_loss = 0.0\n",
    "  \n",
    "  # Iterate over the DataLoader for training data\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    \n",
    "    # Get inputs\n",
    "    inputs, targets = data\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Perform forward pass\n",
    "    outputs = network.forward(inputs)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function(outputs, targets)\n",
    "    \n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Perform optimization\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print statistics\n",
    "    current_loss += loss.item()\n",
    "\n",
    "    \n",
    "  print('Training Loss after epoch %5d: %.3f' %\n",
    "        (epoch+1, current_loss / 300))\n",
    "  current_loss = 0.0\n",
    "\n",
    "  # Calculating test accuracy \n",
    "  accuracy = np.empty(0)\n",
    "  for i, test_data in enumerate(test_loader,0):\n",
    "    inputs, targets = test_data\n",
    "    # making prediction\n",
    "    outputs = network.forward(inputs)\n",
    "    _, predicted = torch.max(outputs.data,1)\n",
    "    # calculate accuracy and add it in an accuracy array, one element for each batch\n",
    "    accuracy = np.append(accuracy, (predicted == targets).sum().item() /predicted.size(0))\n",
    "  print('Test Accuracy after epoch %5d: %.3f' % (epoch+1, accuracy.mean()))\n",
    "\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 7,\n",
       "        8, 9, 0, 8, 3, 9, 5, 5, 2, 6, 8, 4, 1, 7, 1, 2, 3, 5, 6, 9, 1, 1, 1, 2,\n",
       "        1, 2, 0, 7, 7, 5, 8, 2, 9, 8, 6, 7, 3, 4, 6, 8, 7, 0, 4, 2, 7, 7, 5, 4,\n",
       "        3, 4, 2, 8, 1, 5, 1, 0, 2, 3, 3, 5, 7, 0, 6, 8, 6, 3, 9, 9, 8, 2, 7, 7,\n",
       "        1, 0, 1, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 7, 8, 9, 7,\n",
       "        8, 6, 4, 1, 9, 3, 8, 4, 4, 7, 0, 1, 9, 2, 8, 7, 8, 2, 6, 0, 6, 5, 3, 3,\n",
       "        3, 9, 1, 4, 0, 6, 1, 0, 0, 6, 2, 1, 1, 7, 7, 8, 4, 6, 0, 7, 0, 3, 6, 8,\n",
       "        7, 1, 5, 2, 4, 9, 4, 3, 6, 4, 1, 7, 2, 6, 5, 0, 1, 2, 3, 4, 5, 6, 7, 8,\n",
       "        9, 0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Do not change\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# load data\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=200, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=200, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after epoch 1: 1.255459 \n",
      "\n",
      "Test Accuracy after epoch 2: 0.829565 \n",
      "\n",
      "Test Accuracy after epoch 3: 0.671443 \n",
      "\n",
      "Test Accuracy after epoch 4: 0.581313 \n",
      "\n",
      "Test Accuracy after epoch 5: 0.535588 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#MY SOLUTION\n",
    "# your code here\n",
    "\n",
    "#model architecture\n",
    "class NeuralNetwork(nn.Module):  \n",
    " \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28 * 1, 128),      \n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self, x):  \n",
    "        return self.layers(x)\n",
    "    \n",
    "\n",
    "net = NeuralNetwork()\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 1e-4\n",
    "epochs = 5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr =learning_rate)\n",
    "\n",
    "\n",
    "# Define start loop\n",
    "def start(): \n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_loader, net, loss_fn, optimizer)\n",
    "        accuracy = test_loop(test_loader, net, loss_fn)\n",
    "        \n",
    "        print(f\"Test Accuracy after epoch {t+1}: {accuracy:>8f} \\n\")\n",
    "            \n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "# Define training loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "# Define test loop\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item() \n",
    "\n",
    "    test_loss /= num_batches\n",
    "    \n",
    "    return test_loss\n",
    "\n",
    "start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4I37ZrkE3nDb"
   },
   "source": [
    "## 4 Data Augmentation  (1 pts)\n",
    "Read this [article](https://neptune.ai/blog/data-augmentation-nlp) by Shahul Es and  about data augmentation in natural language processing. For a fuller picture, you can also have a look at [this article](https://amitness.com/2020/05/data-augmentation-for-nlp/) by Amit Chaudhary. Answer the following question:\n",
    "\n",
    "- Data augmentation in NLP is very task-specific and should be applied carefully. \n",
    "Provide an example of a augmentation method \\& task pair where data augmentation might harm the model. Justify your answer (2-4 sentences).\n",
    "\tA list of NLP tasks you can find [here](https://natural-language-understanding.fandom.com/wiki/List_of_natural_language_processing_tasks) (you can also give an example with a task not from this list).\n",
    "- Present your findings in the tutorial for bonus points"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "88JQRIxOr8ZW",
    "HGXXAeWiuoex",
    "w2GaX5BPv2TV",
    "xAERZd5-wEi2",
    "qRZRrsXXx_1S",
    "Lf9RD-f2yFRW",
    "wq0pNaOnzSfh",
    "t9rc_tOazUGa",
    "5CVObdLz0EA6",
    "VYy88Ig33gkI"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
