{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5E36qAPf-8f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NNIA Assignment 6\n",
    "\n",
    "**DEADLINE: 21. 12. 2022 08:00 CET**\n",
    "Submission more than 10 minutes past the deadline will **not** be graded!\n",
    "\n",
    "- **Name & ID 1**: Antonia WÃ¤chter, 7007542 (anwa00001)\n",
    "- **Name & ID 2**: Asmaa Ibrahim, 7039914 (asib00001)\n",
    "- **Name & ID 3**: Eva Richter, 2559531 (s8evrich)\n",
    "- **Hours of work per person: 15\n",
    "\n",
    "# Submission Instructions\n",
    "\n",
    "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the assignment deadline. All course-related questions can be addressed on the course **[Piazza Platform](https://piazza.com/class/kvc3vzhsvh55rt)**.\n",
    "\n",
    "* Assignments are to be submitted in a **team of 2 or 3**.\n",
    "* Please include your **names**, **ID's**, **Teams usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
    "* Make sure you appropriately comment your code wherever required.\n",
    "* Your final submission should contain this completed Jupyter Notebook, including the bonus question (if you attempt it), and any necessary Python files.\n",
    "* Do **not** submit any **data or cache files** (e.g. `__pycache__`, the dataset PyTorch downloads, etc.). \n",
    "* Upload the **zipped** folder (*.zip* is the only accepted extension) in **Teams**.\n",
    "* Only **one member** of the group should make the submisssion.\n",
    "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2.zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization epeatedly students fail to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88JQRIxOr8ZW"
   },
   "source": [
    "## Notations\n",
    "\n",
    "- ${A \\in \\mathbb{R}^{m \\times n}}$: capital letter for a matrix. ${m}$ refers to the number of rows and ${n}$ refers to the number of columns.\n",
    "- ${\\textbf{x} \\in \\mathbb{R}^{n \\times 1}}$: bold lowercase for a vector\n",
    "- ${a \\in \\mathbb{R}}$: lowercase for a scalar\n",
    "- ${\\textbf{1}}$: bold one denotes a vector of ones, e.g., ${a \\cdot \\textbf{1} = a \\textbf{1} = [a \\cdot 1,~a \\cdot 1]^{T} = \\begin{bmatrix}a\\\\a\\end{bmatrix} }$.\\\n",
    "Superscipt ${T}$ means \"transpose\": swap rows and columns.\n",
    "- Symbol ${:=}$ means, \"(left-hand side) is defined as (right-hand side)\".\n",
    "- ${f(w;x, y)}$ means that we see it as a function of ${w}$, while ${x}$ and ${y}$ are given (and fixed). Variable ${w}$ can be a trainable parameter in a neural network model, while ${x}$ is an input data and ${y}$ is the ground truth solution for the data point ${x}$. It means that we are going to update ${w}$ during back-propagation, while both ${x}$ and ${y}$ are given (and fixed) data.\n",
    "- Symbol ${\\rightarrow}$ means that the right-hand side of ${\\rightarrow}$ is computed by some operations given the left-hand side.\n",
    "- Symbol ${\\leftrightarrow}$ means an expression on the right-hand side of ${\\leftrightarrow}$ is equivalent to the left-hand side (see below).\n",
    "- Symbol ${ \\circ }$ means a composition of functions: A function on the right-hand side of ${\\circ}$ serves as an input argument to a function on the left-hand side. So, it holds that \\begin{align*} g \\circ f(x) ~\\leftrightarrow~ g(f(x)). \\end{align*}\n",
    "- [Gradient](https://en.wikipedia.org/wiki/Gradient) is generalisation of derivative for a function in higher dimension (many parameters). For example, a function \\begin{align*} f(x_{1}, x_{2}, ..., x_{n}) \\end{align*} may have its gradient \\begin{align*} \\nabla f(x_{1}, x_{2}, ..., x_{n}) = \\left[~\\frac{\\partial}{\\partial x_{1}} f, \\frac{\\partial}{\\partial x_{2}} f, ..., \\frac{\\partial}{\\partial x_{n}} f \\right]^{T}. \\end{align*} Each element in a gradient vector is a [partial derivative](https://en.wikipedia.org/wiki/Partial_derivative).\n",
    "\n",
    "## Definitions and Properties\n",
    "- ${\\ell_{2}}$-norm: ${ {\\lVert{\\textbf{x}}\\rVert}_{2} := \\sqrt{  \\sum_{i = 1}^{n} \\lvert x_{i} \\rvert^{2} } = \\sqrt{ x_{1}^{2} + ..... + x_{n}^{2} } }$\n",
    "- squared ${\\ell_{2}}$-norm: ${ \\lVert{\\textbf{x}}\\rVert_{2}^{2} = x_{1}^{2} + ... + x_{n}^{2} }$,\\\n",
    "so mean squared error (MSE) loss function is a summation of all the squared elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKCd8GiZf-8o"
   },
   "source": [
    "## 1 The Chain Rule in Computational Graphs (2 points)\n",
    "\n",
    "The structure of neural networks is often represented using computational graphs to make complex operations easier to understand. If you are unfamiliar with computational graphs, read this [Intro to Computational Graphs in Deep Learning](https://www.geeksforgeeks.org/computational-graphs-in-deep-learning/) and/or watch these videos on [Computational Graphs](https://youtu.be/hCP1vGoCdYU) and [Derivatives on Compuataional Graphs](https://youtu.be/nJyUyKN-XBQ) by DeeplearningAI before attempting this exercise.  \n",
    "\n",
    "Below is an example of a simple computation graph. Using this, write down the expressions (by applying the chain rule) and calculate the final values for the following partial derivatives:\n",
    "\n",
    "\n",
    "1.   $\\frac{\\partial e}{\\partial b}$\n",
    "2.   $\\frac{\\partial e}{\\partial a}$\n",
    "\n",
    "<img src=\"comp_graph_fixed.png\" alt=\"Computional Graph\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuyyIbBvodg3"
   },
   "source": [
    "## 1 <font color=\"green\">Done</font>\n",
    "\n",
    "\n",
    "$\\frac{\\delta c} {\\delta a} = 1\\newline\n",
    "\\frac{\\delta d} {\\delta a} = 2a = 4\\newline\n",
    "\\frac{\\delta c} {\\delta b} = 2\\newline\n",
    "\\frac{\\delta d} {\\delta b} = 2b = 2\\newline\n",
    "\\frac{\\delta e} {\\delta c} = d = 5\\newline\n",
    "\\frac{\\delta e} {\\delta d} = c = 4\\newline$\n",
    "\n",
    "$c = 2 + (2 \\times 1) = 4 \\newline\n",
    "d = 2^{2} + 1^{2} = 5\\newline\n",
    "e = 4\\times 5 = 20$\n",
    "\n",
    "1. $\\frac{\\delta e}{\\delta b} = \\big(\\frac{\\delta e}{\\delta d} \\times \\frac{\\delta d}{\\delta b}\\big) + \\big(\\frac{\\delta e}{\\delta c} \\times \\frac{\\delta c}{\\delta b}\\big) = (4 \\times 2) + (5 \\times 2) = 18$ \n",
    "\n",
    "2. $\\frac{\\delta e}{\\delta a} = \\big(\\frac{\\delta e}{\\delta d} \\times \\frac{\\delta d}{\\delta a}\\big) + \\big(\\frac{\\delta e}{\\delta c} \\times \\frac{\\delta c}{\\delta a}\\big) = (4 \\times 4) + (5 \\times 1) = 21$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtJIGvCsutKt"
   },
   "source": [
    "## 2 Forward and Backward Propagation (5 + 1 points)\n",
    "\n",
    "Consider a loss function ${l: \\mathbb{R}^{2} \\rightarrow \\mathbb{R}}$\n",
    "\n",
    "\\begin{align*}\n",
    "  l\\left( A, u;~\\textbf{x}, \\textbf{y} \\right) := \\frac{1}{2} \\left\\lVert \\sigma \\left( A \\textbf{x} + u \\textbf{1} \\right) -~\\textbf{y} ~\\right\\rVert_{2}^{2} &&(1)\n",
    "\\end{align*}\n",
    "\n",
    "with trainable parameters\n",
    "\\begin{align*}\n",
    "    A &\\in \\mathbb{R}^{2 \\times 2}, \\\\\n",
    "    u &\\in \\mathbb{R},\n",
    "\\end{align*}\n",
    "and input data point and its ground truth solution\n",
    "\\begin{align*}\n",
    "    \\textbf{x},~\\textbf{y} \\in \\mathbb{R}^{2}.\n",
    "\\end{align*}\n",
    "\n",
    "\\\n",
    "The nonlinear activation (logistics sigmoid from assignment-04)\n",
    "in (1) is defined as\n",
    "\\begin{align*}\n",
    "\\sigma(\\textbf{x}) :=\n",
    "    \\frac{ 1 }{ 1 + e^{-\\textbf{x}} } = \\frac{ e^{\\textbf{x}} }{ e^{\\textbf{x}} + 1 },\n",
    "\\end{align*}\n",
    "which has its gradient (and derivative)\n",
    "\\begin{align*}\n",
    "\\sigma'(\\textbf{x}) &= \\left( 1 - \\sigma(\\textbf{x}) \\right) \\cdot \\sigma(\\textbf{x}),\\\\\n",
    "\\sigma'(x)      &= \\left( 1 - \\sigma(    x ) \\right) \\cdot \\sigma(    x).\n",
    "\\end{align*}\n",
    "\n",
    "Hint. The operation inside this activation function happens elementwise, and there is no change of the dimension of its output compared to the input dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wsZPmwvw9Vb"
   },
   "source": [
    "## 2.1 Forward Propagation in Hidden Layer\n",
    "Specify all the elements in\n",
    "\\begin{align*}\n",
    "    \\sigma \\left( A \\textbf{x} + u \\textbf{1} \\right) \n",
    "    ~\\leftrightarrow~\n",
    "    \\text{Sigmoid} \\;\\circ\\; L_{2 \\rightarrow 2} \n",
    "\\end{align*}\n",
    "given\n",
    "\\begin{align*}\n",
    "    A &= \\begin{bmatrix}\n",
    "        a_{11} & a_{12}\\\\\n",
    "        a_{21} & a_{22}\n",
    "    \\end{bmatrix},\n",
    "    \\textbf{x} = \\begin{bmatrix}\n",
    "        x_{1} \\\\ x_{2}\n",
    "    \\end{bmatrix}, \\textbf{1} = \\begin{bmatrix}\n",
    "        1 \\\\ 1\n",
    "    \\end{bmatrix},\n",
    "\\end{align*}\n",
    "and\n",
    "${u \\in \\mathbb{R}}$ is a scalar.\n",
    "\n",
    "\\\n",
    "You do not need to use an explicit expression of the activation function that comprises of exponential and fraction. It is sufficient if you use an implicit notation ${\\sigma \\left( \\bullet \\right)}$, with symbol ${\\bullet}$ meaning its input argument \n",
    "that you must find out and show. \n",
    "\n",
    "Comment. ${L_{2 \\rightarrow 2}}$ is a non-standard expression since it only tells that it is a linear map (from two inputs to two outputs), however, without saying how such a linear operation is computed. It is put here only for people who would feel more friendly with this kind of expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad5MalTXyiRy"
   },
   "source": [
    "## 2.2 Forward Propagation towards Loss Function\n",
    "\n",
    "The loss function (1)\n",
    "is a scalar-valued function, which gives a real number.\n",
    "Show the explicit expression of this loss function using the previous\n",
    "result that you found.\n",
    "Keep an implicit expression ${\\sigma \\left( \\bullet \\right)}$\n",
    "and use the following notation for the ground truth:\n",
    "\\begin{align*}\n",
    "\\textbf{y} = \\begin{bmatrix}\n",
    "    y_{1} \\\\ y_{2}\n",
    "\\end{bmatrix}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB7nMIjWyt33"
   },
   "source": [
    "## 2.3 A Backward Propagation - I\n",
    "\n",
    "Compute the [partial derivative](https://en.wikipedia.org/wiki/Partial_derivative) of the loss \n",
    "function (1) w.r.t. (with respect to) the parameter ${a_{11}}$. \n",
    "For the derivative of the logistics sigmoid activation function,\n",
    "use the given rule:\n",
    "\\begin{align*}\n",
    "\\sigma'(x) &= \\left( 1 - \\sigma( x ) \\right) \\cdot \\sigma( x ).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn-5XjD9y7N-"
   },
   "source": [
    "## 2.4 A Backward Propagation - II\n",
    "\n",
    "Compute the partial derivative of the loss \n",
    "function (1) w.r.t. the parameter  ${a_{12}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g29eRiK4zFUa"
   },
   "source": [
    "## 2.5 A Backward Propagation - III\n",
    "Compute the partial derivative of the loss \n",
    "function (1) w.r.t. the parameter ${u}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zwIDuXKzOcv"
   },
   "source": [
    "## 2.6 Bonus Questions\n",
    "- How many trainable parameters this model (1) has?\n",
    "- Why would the partial derivative expression in 2.5 look as twice long as the the partial derivative in 2.4?\n",
    "- Would the loss function (1) and its gradient will stay the same when the input data ${\\textbf{x}}$ changes? What would it mean in the case of a neural network training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAHN9Y9dodg-"
   },
   "source": [
    "## 2 <font color=\"green\">Done</font>\n",
    "\n",
    "$\\bf{2.1}$ \n",
    "\n",
    "$ \\hat{y} = \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\end{bmatrix} := \\sigma ( A\\textbf{x} + u \\textbf{1}) = \\begin{bmatrix}\\sigma(a_{11}x_1+a_{12}x_2+u) \\\\ \\sigma(a_{21}x_1+a_{22}x_2+u)\\end{bmatrix} $ <br>\n",
    "\n",
    "$h_1=a_{11}x_1+a_{12}x_2+u$ <br>\n",
    "$h_2=a_{21}x_1+a_{22}x_2+u$ <br>\n",
    "\n",
    "$\\bf{2.2}$ \n",
    "\n",
    "$l( A, u;~\\textbf{x}, \\textbf{y} )  := \\frac{1}{2} \\lVert \\sigma ( A \\textbf{x} + u \\textbf{1} ) -~\\textbf{y} ~\\rVert_{2}^{2} \\frac{1}{2}(\\hat{y}_1-y_1)^2+\\frac{1}{2}(\\hat{y}_2-y_2)^2+\\frac{1}{2}(\\sigma(a_{11}x_1+a_{12}x_2+u)-y_1)^2+\\frac{1}{2}(\\sigma(a_{21}x_1+a_{22}x_2+u)-y_2)^2$\n",
    "\n",
    "\n",
    "$\\bf{2.3}$ \n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_{11}} = \\frac{\\partial l}{\\partial \\hat{y}_1} * \\frac{\\partial \\hat{y}_1}{\\partial h_1} * \\frac{\\partial h_1}{\\partial a_{11}} = (\\hat{y}_1 - y_1) *(1-\\sigma(h_1))*\\sigma(h_1) * x_1 = (\\sigma(a_{11}x_1+a_{12}x_2+u)-y_1)*(1-\\sigma(a_{11}x_1+a_{12}x_2+u))*(\\sigma(a_{11}x_1+a_{12}x_2+u))* x_1$ <br>\n",
    "\n",
    "$\\bf{2.4}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_{12}} =  \\frac{\\partial l}{\\partial \\hat{y}_2} * \\frac{\\partial \\hat{y}_2}{\\partial h_2} * \\frac{\\partial h_2}{\\partial a_{12}} = (\\sigma(a_{11}x_{1} + a_{12}x_{2} + u) - y_{1})* (1 - \\sigma(a_{11}x_{1} + a_{12}x_{2} + u)) * (\\sigma(a_{11}x_{1} + a_{12}x_{2} + u))* x_{2}$\n",
    "\n",
    "$\\bf{2.5}$\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial u} =  \\frac{\\partial l}{\\partial \\hat{y}_{1}} * \\frac{\\partial \\hat{y}_{1}}{\\partial h_{1}} * \\frac{\\partial {h}_{1}}{\\partial u} + \\frac{\\partial l}{\\partial \\hat{y}_{2}} * \\frac{\\partial \\hat{y}_{2}}{\\partial {h}_{2}} * \\frac{\\partial {h}_{2}}{\\partial u} = (\\sigma(a_{11}x_{1} + a_{12}x_{2} + u) - y_{1}) * (1 - \\sigma(a_{11}x_{1} + a_{12}x_{2} + u)) * (\\sigma(a_{11}x_{1} + a_{12}x_{2} + u)) + (\\sigma(a_{21}x_{1} + a_{22}x_{2} + u) - y_{2}) * (1 - \\sigma(a_{21}x_{1} + a_{22}x_{2} + u)) * (\\sigma(a_{21}x_{1} + a_{22}x_{2} + u))$\n",
    "\n",
    "$\\bf{2.6}$\n",
    "\n",
    "- There are 5 trainable parameters: four weights $(a_{11}, a_{12},a_{21}, a_{22})$ and one bias (u).\n",
    "- This is because for computing the partial derivative expression of the loss w.r.t. $a_{12}$ we only need to take into account one output, which is in this case $\\hat{y}_{2}$ (and respectively in 2.3 the partial derivative expression of the loss w.r.t. $a_{11}$, which is $\\hat{y}_{1}$). However, to compute the partial derivative expression of the loss w.r.t to the bias $\\textit{u}$ both outputs $\\hat{y}_{1}$ and $\\hat{y}_{2}$ need to be taken into account.\n",
    "- When the input data $\\textbf{x}$ changes the loss function and gradient are still computed in the same way, however they take on different values and hence the updates of the parameters vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWdt7xnbzuja"
   },
   "source": [
    "## 3. Implementation (3 points)\n",
    "Now you will implement the network and the computations from exercise 2 by completing the skeleton code in solution.py. You may not use any external libraries besides numpy. Your tasks are to:  \n",
    " 1. Implement the logistic sigmoid function and its gradient. (0.25 pts)\n",
    " 2. Implement the forward pass and print the network output $\\hat{\\textbf{y}} := \\sigma \\left( A \\textbf{x} + u \\textbf{1} \\right)$ for the dummy input $\\textbf{x}$ given by the seed number zero. Make sure your forward function also returns return the result of the intermediate step $\\textbf{h}$ (i.e. before sigmoid is applied), you will need it afterwards. (1 pt)\n",
    " 3. Implemet the loss function from exercise 2 and compute loss for $\\hat{\\textbf{y}}$ and the dummy ground truth $\\textbf{y}$. (0.25 pts)\n",
    " 4. Implement the backward pass and compute the gradient given $\\textbf{x}$, $\\textbf{y}$ and $h$ for all trainable parameters (Matrix $A$ and bias $u$). (1.5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bItMvinuodg_",
    "outputId": "c366250c-7e80-43ab-dd2a-9bc724a63321"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [[0.5488135 ]\n",
      " [0.71518937]]\n",
      "y = [[0.60276338]\n",
      " [0.54488318]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "\n",
    "np.random.seed(0)  # DO NOT CHANGE\n",
    "\n",
    "x = rand(2,1) # dummy input\n",
    "y = rand(2,1) # dummy y / ground truth\n",
    "\n",
    "print(\"x = \" + str(x))\n",
    "print(\"y = \" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uFCGGafOodhB",
    "outputId": "3e1ff272-ec1c-48fe-a713-0e30f0836fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "h: [[1.65810684]\n",
      " [1.8416031 ]]\n",
      "z: [[0.8399837 ]\n",
      " [0.86313819]]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from solution import Network\n",
    "\n",
    "#initialize the network\n",
    "network = Network(2, 2)\n",
    "#run the forward pass on x and print the result\n",
    "\n",
    "h, z = network.forward(x)\n",
    "print(\"h:\", h)\n",
    "print(\"z:\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BBgFeSksodhC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07877986778215697\n"
     ]
    }
   ],
   "source": [
    "# compute loss\n",
    "loss = network.loss(y, z)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9llg09L4odhC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.01749894, 0.02280384],\n",
      "       [0.02063301, 0.02688802]]), array([0.06948071]))\n"
     ]
    }
   ],
   "source": [
    "# perform backward pass\n",
    "backprop = network.backward(x,y,h)\n",
    "print(backprop)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
